{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4508,"sourceType":"datasetVersion","datasetId":138}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [markdown]\n# # Movie Recommendation System (Content-Based)\n# \n# **Followed steps:** \n# 1. Load TMDB dataset CSVs\n# 2. Preprocess (parse JSON-like fields, handle missing values, build feature \"soup\")\n# 3. Build TF-IDF + cosine similarity content-based recommender\n# 4. (Optional) If `data/ratings.csv` is present, build a simple user-based collaborative recommender\n# 5. Provide `recommend_by_title(title, n=5)` and `recommend_for_user(user_id, n=5)` functions\n#\n# **Data files required (place inside `data/`):**\n# - tmdb_5000_movies.csv\n# - tmdb_5000_credits.csv\n# - (optional) ratings.csv  --> columns: user_id, movie_id, rating   (movie_id should match tmdb movie 'id')\n#\n# Run the cells below in order.\n\n# %% [markdown]\n# ## 1. Imports & helpers\n\n# %%\nimport os\nimport ast\nimport json\nfrom typing import List, Optional\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# %% [markdown]\n# ## 2. File paths\n\n# %%\nDATA_DIR = \"data\"\nMOVIES_CSV = os.path.join(DATA_DIR, \"/kaggle/input/tmdb-movie-metadata/tmdb_5000_movies.csv\")\nCREDITS_CSV = os.path.join(DATA_DIR, \"/kaggle/input/tmdb-movie-metadata/tmdb_5000_credits.csv\")\nRATINGS_CSV = os.path.join(DATA_DIR, \"/kaggle/input/ratings.csv\")  # optional\n\n# %% [markdown]\n# ## 3. JSON-parsing helper (TMDB stores many fields as strings that look like lists/dicts)\n# Safe parsing with fallback.\n\n# %%\ndef try_parse_json(x):\n    \"\"\"Safely parse JSON-like strings stored in CSV (returns list/dict or [] if not parseable).\"\"\"\n    if pd.isna(x):\n        return []\n    if isinstance(x, (list, dict)):\n        return x\n    try:\n        return ast.literal_eval(x)\n    except Exception:\n        try:\n            return json.loads(x)\n        except Exception:\n            return []\n\n# %% [markdown]\n# ## 4. Load & merge TMDB files\n# - `tmdb_5000_movies.csv` contains movie metadata (genres, keywords, overview, etc.)\n# - `tmdb_5000_credits.csv` contains cast & crew (movie_id -> id). We'll merge on `id`.\n\n# %%\ndef load_tmdb(movies_fp=MOVIES_CSV, credits_fp=CREDITS_CSV) -> pd.DataFrame:\n    if not (os.path.exists(movies_fp) and os.path.exists(credits_fp)):\n        raise FileNotFoundError(f\"Missing TMDB files in '{DATA_DIR}'. Please add tmdb_5000_movies.csv and tmdb_5000_credits.csv\")\n    movies = pd.read_csv(movies_fp)\n    credits = pd.read_csv(credits_fp).rename(columns={'movie_id':'id'})\n    df = movies.merge(credits[['id','cast','crew']], on='id', how='left')\n\n    # parse JSON-like columns\n    for col in ['genres','keywords','cast','crew']:\n        if col in df.columns:\n            df[col] = df[col].apply(try_parse_json)\n    # ensure overview column exists\n    if 'overview' not in df.columns:\n        df['overview'] = ''\n    print(f\"Loaded dataframe with {len(df)} movies.\")\n    return df\n\n# Load\ndf = load_tmdb()\ndf.head()[['id','title','genres','keywords','cast']]\n\n# %% [markdown]\n# ## 5. Preprocessing helpers: extract names/norm text and build 'soup'\n# - We extract genre names, keyword names, top-k cast names, director name\n# - We lower-case and remove spaces inside tokens (so \"Tom Hanks\" -> \"tomhanks\") which helps token matching\n# - Combine fields into a weighted string called `soup`\n\n# %%\ndef names_from_list(ld, key='name', top_n: Optional[int]=None) -> List[str]:\n    if not isinstance(ld, list): \n        return []\n    out = []\n    for item in ld:\n        if isinstance(item, dict):\n            val = item.get(key) or item.get('title') or item.get('name')\n            if val:\n                out.append(str(val).replace(' ', '').lower())\n        elif isinstance(item, str):\n            out.append(item.replace(' ', '').lower())\n    if top_n:\n        return out[:top_n]\n    return out\n\ndef director_from_crew(crew_list):\n    if not isinstance(crew_list, list):\n        return ''\n    for d in crew_list:\n        if isinstance(d, dict):\n            if (d.get('job') or '').lower() == 'director':\n                return str(d.get('name','')).replace(' ','').lower()\n    return ''\n\ndef build_soup_row(row, cast_top_n=3, weights=None):\n    if weights is None:\n        weights = {'genres':3, 'keywords':2, 'cast':2, 'director':2, 'overview':1}\n    genres = names_from_list(row.get('genres', []))\n    keywords = names_from_list(row.get('keywords', []))\n    cast = names_from_list(row.get('cast', []), top_n=cast_top_n)\n    director = director_from_crew(row.get('crew', []))\n    overview = str(row.get('overview','') or '').lower()\n    parts = []\n    parts += genres * weights.get('genres',1)\n    parts += keywords * weights.get('keywords',1)\n    parts += cast * weights.get('cast',1)\n    if director:\n        parts += [director] * weights.get('director',1)\n    # append overview once (it may be long and is useful)\n    parts.append(overview)\n    return \" \".join(parts)\n\n# Build soups for all movies\ndf['soup'] = df.apply(lambda r: build_soup_row(r, cast_top_n=3), axis=1)\n# quick check\ndf[['title','soup']].head(3)\n\n# %% [markdown]\n# ## 6. Vectorize & compute similarity (TF-IDF + Cosine)\n# - We create a TF-IDF matrix over the `soup` and compute pairwise cosine similarity.\n# - For ~5000 movies this is fine; adjust `max_features` if you need smaller memory footprint.\n\n# %%\ntfidf = TfidfVectorizer(stop_words='english', max_features=5000)\ntfidf_matrix = tfidf.fit_transform(df['soup'].fillna(''))\ncosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)  # dense matrix; size NxN where N = number of movies\n\n# build mapping title -> index for fast lookup\ntitle_to_index = pd.Series(df.index, index=df['title']).to_dict()\n\n# %% [markdown]\n# ## 7. Recommendation function (content-based)\n# - `recommend_by_title(title, n=5)` returns top-n movies most similar to the provided `title`.\n# - The function also tries a case-insensitive fallback if exact title isn't found.\n\n# %%\ndef recommend_by_title(title: str, n: int = 5, show_scores: bool = False) -> pd.DataFrame:\n    # exact title check\n    if title not in title_to_index:\n        # case-insensitive fallback\n        lowered = {k.lower(): v for k,v in title_to_index.items()}\n        if title.lower() in lowered:\n            idx = lowered[title.lower()]\n        else:\n            # try partial matching suggestions\n            candidates = [t for t in title_to_index.keys() if title.lower() in t.lower()]\n            if candidates:\n                # return suggestion list\n                return pd.DataFrame({\"suggestions\": candidates[:10]})\n            return pd.DataFrame({\"error\": [f\"Title '{title}' not found in dataset (try exact title).\"]})\n    else:\n        idx = title_to_index[title]\n    sim_scores = list(enumerate(cosine_sim[idx]))\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n    # exclude the movie itself\n    sim_scores = [s for s in sim_scores if s[0] != idx]\n    top_n = sim_scores[:n]\n    movie_indices = [i for i,_ in top_n]\n    res = df.iloc[movie_indices][['id','title','overview']].copy().reset_index(drop=True)\n    if show_scores:\n        res['score'] = [s for _,s in top_n]\n    return res\n\n# %% [markdown]\n# ## 8. (Optional) Collaborative recommender if you provide ratings.csv\n# - A simple user-based CF: build user-item matrix, compute user-user cosine similarity,\n#   predict top-n unrated movies for a user using weighted average of neighbors' ratings.\n#\n# **ratings.csv format:** columns `user_id,movie_id,rating` where `movie_id` corresponds to TMDB `id` column.\n\n# %%\ndef load_ratings(ratings_fp=RATINGS_CSV) -> Optional[pd.DataFrame]:\n    if not os.path.exists(ratings_fp):\n        print(\"No ratings.csv found in data/ -> collaborative recommender disabled.\")\n        return None\n    r = pd.read_csv(ratings_fp)\n    if not {'user_id','movie_id','rating'}.issubset(set(r.columns)):\n        raise ValueError(\"ratings.csv must contain columns: user_id, movie_id, rating\")\n    print(f\"Loaded ratings with {len(r)} rows from {ratings_fp}\")\n    return r\n\nratings = load_ratings()\n\n# Build CF model if ratings present\nif ratings is not None:\n    # pivot: rows users, cols movie_id, values rating\n    user_item = ratings.pivot_table(index='user_id', columns='movie_id', values='rating')\n    user_item_filled = user_item.fillna(0)\n    user_ids = user_item_filled.index.tolist()\n    # compute user-user similarity\n    from sklearn.metrics.pairwise import cosine_similarity as cos_sim\n    user_sim_matrix = cos_sim(user_item_filled)  # shape (num_users, num_users)\n    user_sim = pd.DataFrame(user_sim_matrix, index=user_ids, columns=user_ids)\n    print(\"Built user-user similarity matrix for collaborative recommender.\")\n\ndef recommend_for_user(user_id: int, n: int = 5) -> pd.DataFrame:\n    if ratings is None:\n        return pd.DataFrame({\"error\":[ \"No ratings.csv provided. Collaborative recommendations require user ratings.\" ]})\n    if user_id not in user_sim.index:\n        return pd.DataFrame({\"error\":[ f\"User {user_id} not found in ratings data.\" ]})\n    # weighted sum of other users' ratings\n    sim_scores = user_sim.loc[user_id]  # similarity to every user\n    weighted_sum = user_item_filled.T.dot(sim_scores)  # series indexed by movie_id\n    denom = sim_scores.abs().sum()\n    if denom == 0:\n        predicted = weighted_sum * 0\n    else:\n        predicted = weighted_sum / denom\n    # remove movies the user has already rated\n    rated_movies = user_item.loc[user_id].dropna().index.tolist() if user_id in user_item.index else []\n    predicted = predicted.drop(index=rated_movies, errors='ignore')\n    # top n movie_ids\n    top_movie_ids = predicted.sort_values(ascending=False).head(n).index.tolist()\n    # map movie_id to titles using df (df.id column)\n    out = df[df['id'].isin(top_movie_ids)][['id','title']].copy().reset_index(drop=True)\n    return out\n\n# %% [markdown]\n# ## 9. Demonstrations\n# - We'll pick a few movie titles from the dataset and show top-5 content-based recommendations.\n# - If you provided `ratings.csv`, we'll also show a demo for a user from that file.\n\n# %%\n# Show a few example titles available in dataset (first 12)\nprint(\"Some movie titles in dataset (first 12):\")\nprint(df['title'].head(12).tolist())\n\n# Choose example titles to demonstrate (use existing titles from dataset)\nexamples = df['title'].iloc[0:3].tolist()  # first 3 movies in dataset (real TMDB)\nfor t in examples:\n    print(\"\\n\" + \"=\"*80)\n    print(\"Query title:\", t)\n    recs = recommend_by_title(t, n=5, show_scores=True)\n    if 'error' in recs.columns:\n        print(recs['error'].iloc[0])\n    elif 'suggestions' in recs.columns:\n        print(\"Did you mean one of these?\\n\", recs['suggestions'].tolist())\n    else:\n        for i,row in recs.iterrows():\n            print(f\"{i+1}. {row['title']}  | score: {row.get('score', 0):.4f}\")\n\n# If ratings exist, demo recommend_for_user\nif ratings is not None:\n    sample_users = ratings['user_id'].unique()[:3]\n    for u in sample_users:\n        print(\"\\n\" + \"-\"*60)\n        print(\"Recommendations for user:\", u)\n        print(recommend_for_user(u, n=5))\n\n# %% [markdown]\n# ## 10. Notes & Submission checklist\n# - Save this notebook as `Movie_Recommender_ContentBased.ipynb` for submission.\n# - In your README (one paragraph), mention:\n#   - You used TMDB metadata (`tmdb_5000_movies.csv`, `tmdb_5000_credits.csv`) and TF-IDF-based content filtering.\n#   - Preprocessing steps: parsed JSON-like columns, extracted genres/keywords/cast/director, built weighted \"soup\".\n#   - How to run: place CSVs in `data/`, run notebook cells.\n# - If you want, I can convert this notebook to a `.ipynb` file and give it to you; otherwise copy/paste into a notebook editor and run.\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T16:17:49.477086Z","iopub.execute_input":"2025-09-08T16:17:49.477478Z","iopub.status.idle":"2025-09-08T16:17:57.093420Z","shell.execute_reply.started":"2025-09-08T16:17:49.477446Z","shell.execute_reply":"2025-09-08T16:17:57.091882Z"}},"outputs":[{"name":"stdout","text":"Loaded dataframe with 4803 movies.\nNo ratings.csv found in data/ -> collaborative recommender disabled.\nSome movie titles in dataset (first 12):\n['Avatar', \"Pirates of the Caribbean: At World's End\", 'Spectre', 'The Dark Knight Rises', 'John Carter', 'Spider-Man 3', 'Tangled', 'Avengers: Age of Ultron', 'Harry Potter and the Half-Blood Prince', 'Batman v Superman: Dawn of Justice', 'Superman Returns', 'Quantum of Solace']\n\n================================================================================\nQuery title: Avatar\n1. Aliens  | score: 0.2842\n2. Star Trek Into Darkness  | score: 0.2588\n3. Barbarella  | score: 0.2066\n4. Predators  | score: 0.2046\n5. Falcon Rising  | score: 0.2042\n\n================================================================================\nQuery title: Pirates of the Caribbean: At World's End\n1. Pirates of the Caribbean: Dead Man's Chest  | score: 0.5220\n2. Pirates of the Caribbean: The Curse of the Black Pearl  | score: 0.3359\n3. Pirates of the Caribbean: On Stranger Tides  | score: 0.2346\n4. Life of Pi  | score: 0.2238\n5. Nim's Island  | score: 0.2152\n\n================================================================================\nQuery title: Spectre\n1. Skyfall  | score: 0.4418\n2. Quantum of Solace  | score: 0.4220\n3. Never Say Never Again  | score: 0.2822\n4. Restless  | score: 0.2415\n5. Casino Royale  | score: 0.2316\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}